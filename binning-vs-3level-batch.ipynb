{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import random\n",
    "import bz2\n",
    "import cPickle, dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "class Indexer(object):\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in xrange(0, len(self))])\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "    def get_object(self, index):\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "    def contains(self, object):\n",
    "        return self.index_of(object) != -1\n",
    "    def index_of(self, object):\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "    def get_index(self, object, add=True):\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#triples = 3062\n",
      "#nouns = 435\n",
      "#verbs = 156\n"
     ]
    }
   ],
   "source": [
    "# Load triples\n",
    "\n",
    "data_dir = '/home/jacobsuwang/Documents/UTA2017/PROPERTIES/AMT/TRIPLES/'\n",
    "\n",
    "n_indexer = Indexer()\n",
    "v_indexer = Indexer()\n",
    "\n",
    "manual_correct = {'monkeu':'monkey','shir':'shirt',\n",
    "                  'moneky':'monkey','egges':'egg',\n",
    "                  'grap':'grape','woool':'wool',\n",
    "                  'chelf':'chef','gril':'girl',\n",
    "                  'dophin':'dolphin','iar':'air',\n",
    "                  'bowling-bell':'bowling','milke':'milk',\n",
    "                  'knie':'knife','skyscaper':'skyscraper',\n",
    "                  'borse':'horse','blook':'blood',\n",
    "                  'dew':'sew','spetula':'spatula','skycraper':'skyscraper',\n",
    "                  'bowling-ball':'bowling','airoplane':'aeroplane'}\n",
    "def correct_word(word):\n",
    "    return word if word not in manual_correct else manual_correct[word]\n",
    "\n",
    "svo2plau = dict()\n",
    "def read_from_triples(path, val, svo2plau):\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if len(line)!=3: continue\n",
    "            s,v,o = line\n",
    "            s,v,o = correct_word(s),correct_word(v),correct_word(o)\n",
    "            v_indexer.get_index(v)\n",
    "            n_indexer.get_index(s)\n",
    "            n_indexer.get_index(o)\n",
    "            svo2plau[(s,v,o)] = val\n",
    "read_from_triples(data_dir+'pos-all.txt', 1, svo2plau)\n",
    "read_from_triples(data_dir+'neg-all.txt', 0, svo2plau)\n",
    "\n",
    "print '#triples = ' + repr(len(svo2plau))\n",
    "print '#nouns = ' + repr(len(n_indexer))\n",
    "print '#verbs = ' + repr(len(v_indexer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load features\n",
    "\n",
    "feat_path = '/home/jacobsuwang/Documents/UTA2017/PROPERTIES/AMT/FEATURES/nouns.xlsx'\n",
    "\n",
    "FEAT_LIST = ['SIZE','WEIGHT','RIGIDITY','SENTIENCE','MASS-COUNT','PHASE']\n",
    "\n",
    "featbindiff_indexer = Indexer()\n",
    "feat3lvdiff_indexer = Indexer()\n",
    "\n",
    "noun2feat2val = defaultdict(lambda : dict())    \n",
    "def read_feat(path, feat_type, noun2feat2val):\n",
    "    df = pd.read_excel(path, sheetname=feat_type)\n",
    "    for i in xrange(len(df)):\n",
    "        entry = df.iloc[i]\n",
    "        noun = str(entry['WORDS'])\n",
    "        feat_val = np.argmax(list(entry)[1:])\n",
    "        noun2feat2val[correct_word(noun)][feat_type] = feat_val\n",
    "        \n",
    "for feat_type in FEAT_LIST:\n",
    "    read_feat(feat_path, feat_type, noun2feat2val)\n",
    "    df = pd.read_excel(feat_path, sheetname=feat_type)\n",
    "    num_feat = len(df.columns[1:])\n",
    "    for diff in np.arange(-(num_feat-1), num_feat):\n",
    "        featbindiff_indexer.get_index(feat_type+':'+str(diff))\n",
    "    for lv in np.arange(-1, 2):\n",
    "        feat3lvdiff_indexer.get_index(feat_type+':'+str(lv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 400000\n"
     ]
    }
   ],
   "source": [
    "# Load glove\n",
    "\n",
    "glove_path = '/home/jacobsuwang/Documents/UTA2017/PROPERTIES/DATA4/glove.6B.300d.txt'\n",
    "\n",
    "word2emb = {}\n",
    "\n",
    "with open(glove_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        word2emb[line[0]] = np.asarray(map(np.float, line[1:]))\n",
    "\n",
    "print 'vocab size = ' + repr(len(word2emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SIZE:-6', 'SIZE:-5', 'SIZE:-4', 'SIZE:-3', 'SIZE:-2', 'SIZE:-1', 'SIZE:0', 'SIZE:1', 'SIZE:2', 'SIZE:3', 'SIZE:4', 'SIZE:5', 'SIZE:6', 'WEIGHT:-6', 'WEIGHT:-5', 'WEIGHT:-4', 'WEIGHT:-3', 'WEIGHT:-2', 'WEIGHT:-1', 'WEIGHT:0', 'WEIGHT:1', 'WEIGHT:2', 'WEIGHT:3', 'WEIGHT:4', 'WEIGHT:5', 'WEIGHT:6', 'RIGIDITY:-4', 'RIGIDITY:-3', 'RIGIDITY:-2', 'RIGIDITY:-1', 'RIGIDITY:0', 'RIGIDITY:1', 'RIGIDITY:2', 'RIGIDITY:3', 'RIGIDITY:4', 'SENTIENCE:-5', 'SENTIENCE:-4', 'SENTIENCE:-3', 'SENTIENCE:-2', 'SENTIENCE:-1', 'SENTIENCE:0', 'SENTIENCE:1', 'SENTIENCE:2', 'SENTIENCE:3', 'SENTIENCE:4', 'SENTIENCE:5', 'MASS-COUNT:-3', 'MASS-COUNT:-2', 'MASS-COUNT:-1', 'MASS-COUNT:0', 'MASS-COUNT:1', 'MASS-COUNT:2', 'MASS-COUNT:3', 'PHASE:-2', 'PHASE:-1', 'PHASE:0', 'PHASE:1', 'PHASE:2']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featbindiff_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for getting features and embs\n",
    "\n",
    "def get_glove(word):\n",
    "    return word2emb[word] if word in word2emb else np.zeros(300)\n",
    "\n",
    "def to_3lv_feat(diff):\n",
    "    if diff>0:\n",
    "        return 1\n",
    "    elif diff==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def get_features(s, o, scheme='bin'):\n",
    "    assert scheme in ['bin','3lv']\n",
    "    so_fts = []\n",
    "    for feat_type in FEAT_LIST:\n",
    "        s_ft = noun2feat2val[s][feat_type]\n",
    "        o_ft = noun2feat2val[o][feat_type]\n",
    "        diff = s_ft - o_ft\n",
    "        feat_name = feat_type+':'+str(diff) if scheme=='bin' \\\n",
    "                    else feat_type+':'+str(to_3lv_feat(diff))\n",
    "        so_fts.append(featbindiff_indexer.get_index(feat_name,add=0)\n",
    "                        if scheme=='bin' else\n",
    "                        feat3lvdiff_indexer.get_index(feat_name,add=0))\n",
    "    return so_fts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format input\n",
    "\n",
    "scheme = 'bin'\n",
    "\n",
    "X_so, X_v, Y = [], [], []\n",
    "X_gl = []\n",
    "for (s,v,o),plau in svo2plau.iteritems():\n",
    "    X_so.append(get_features(s,o,scheme))\n",
    "    X_v.append(v_indexer.get_index(v, add=0))\n",
    "    X_gl.append(np.concatenate((get_glove(s),get_glove(v),get_glove(o))))\n",
    "    Y.append(plau)\n",
    "    \n",
    "X_so = np.asarray(X_so)\n",
    "X_v = np.reshape(np.asarray(X_v),[-1,1])\n",
    "X_gl = np.asarray(X_gl)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "class DataIteratorEmb:\n",
    "    \n",
    "    def __init__(self, X_gl, Y):\n",
    "        self.X_gl = deepcopy(X_gl)\n",
    "        self.Y = deepcopy(Y)\n",
    "        self.size = len(X_gl)\n",
    "        self.indices = np.arange(self.size)\n",
    "        self.epoch = 0\n",
    "        self.cursor = 0\n",
    "        self.shuffle()\n",
    "    \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.indices)\n",
    "        self.X_gl = self.X_gl[self.indices]\n",
    "        self.Y = self.Y[self.indices]\n",
    "        self.cursor = 0\n",
    "    \n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epoch += 1\n",
    "            self.shuffle()    \n",
    "        X_gl_batch = self.X_gl[self.cursor:self.cursor+n]\n",
    "        Y_batch = self.Y[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return X_gl_batch, Y_batch\n",
    "    \n",
    "\n",
    "class DataIteratorEmbWK:\n",
    "    \n",
    "    def __init__(self, X_so, X_v, X_gl, Y):\n",
    "        self.X_so = deepcopy(X_so)\n",
    "        self.X_v = deepcopy(X_v)\n",
    "        self.X_gl = deepcopy(X_gl)\n",
    "        self.Y = deepcopy(Y)\n",
    "        self.size = len(X_so)\n",
    "        self.indices = np.arange(self.size)\n",
    "        self.epoch = 0\n",
    "        self.cursor = 0\n",
    "        self.shuffle()\n",
    "    \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.indices)\n",
    "        self.X_so = self.X_so[self.indices]\n",
    "        self.X_v = self.X_v[self.indices]\n",
    "        self.X_gl = self.X_gl[self.indices]\n",
    "        self.Y = self.Y[self.indices]\n",
    "        self.cursor = 0\n",
    "    \n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epoch += 1\n",
    "            self.shuffle()    \n",
    "        X_so_batch = self.X_so[self.cursor:self.cursor+n]\n",
    "        X_v_batch = self.X_v[self.cursor:self.cursor+n]\n",
    "        X_gl_batch = self.X_gl[self.cursor:self.cursor+n]\n",
    "        Y_batch = self.Y[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return X_so_batch, X_v_batch, X_gl_batch, Y_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 98.66\n",
      "Epoch 1 loss = 92.50\n",
      "Epoch 2 loss = 88.92\n",
      "Epoch 3 loss = 86.03\n",
      "Epoch 4 loss = 82.91\n",
      "Epoch 5 loss = 80.87\n",
      "Epoch 6 loss = 78.47\n",
      "Epoch 7 loss = 77.13\n",
      "Epoch 8 loss = 75.42\n",
      "Epoch 9 loss = 74.76\n",
      "Epoch 10 loss = 73.55\n",
      "Epoch 11 loss = 72.79\n",
      "Epoch 12 loss = 71.69\n",
      "Epoch 13 loss = 71.35\n",
      "Epoch 14 loss = 70.44\n",
      "Epoch 15 loss = 69.97\n",
      "Epoch 16 loss = 69.65\n",
      "Epoch 17 loss = 69.08\n",
      "Epoch 18 loss = 68.49\n",
      "Epoch 19 loss = 68.36\n",
      "\n",
      "Accuracy on train: 0.77241379310344827\n",
      "Accuracy on train: 0.68078175895765469\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "num_epochs = 20\n",
    "batch_size = 20\n",
    "\n",
    "emb_size = 10\n",
    "hidden_size = 10\n",
    "glove_size = 900\n",
    "num_classes = 2\n",
    "\n",
    "init_lr = 1e-4\n",
    "\n",
    "def get_feat_size(scheme):\n",
    "    return len(featbindiff_indexer) if scheme=='bin' else len(feat3lvdiff_indexer)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_gl = tf.placeholder(tf.float32, [None, glove_size])\n",
    "\n",
    "W1 = tf.get_variable('W1', [glove_size, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable('b1', [hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "z = tf.nn.relu(tf.add(tf.matmul(x_gl, W1), b1))\n",
    "\n",
    "W2 = tf.get_variable('W2', [hidden_size, num_classes],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable('b2', [num_classes],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "probs = tf.nn.softmax(tf.add(tf.matmul(z, W2), b2))\n",
    "preds = tf.cast(tf.argmax(probs, 1), dtype=tf.int32)\n",
    "\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "y_onehot = tf.one_hot(y, 2)\n",
    "\n",
    "loss = tf.reduce_mean(tf.negative(tf.log(tf.reduce_sum(tf.multiply(probs, y_onehot), 1))))\n",
    "\n",
    "# eq = tf.equal(preds, y)\n",
    "correct = tf.reduce_sum(tf.cast(tf.equal(preds, y), dtype=tf.int32))\n",
    "\n",
    "decay_steps = 100\n",
    "learning_rate_decay_factor = 0.95\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "initial_learning_rate = init_lr\n",
    "lr = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                global_step,\n",
    "                                decay_steps,\n",
    "                                learning_rate_decay_factor,\n",
    "                                staircase=True)\n",
    "# optimizer setup\n",
    "opt = tf.train.AdamOptimizer(lr)\n",
    "grads = opt.compute_gradients(loss)\n",
    "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "with tf.control_dependencies([apply_gradient_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "# run\n",
    "init = tf.global_variables_initializer()\n",
    "cutoff = int(len(svo2plau)*train_ratio)\n",
    "init_indices = np.arange(len(svo2plau))\n",
    "random.shuffle(init_indices)\n",
    "X_gl = X_gl[init_indices]\n",
    "Y = Y[init_indices]\n",
    "X_gl_train, Y_train = X_gl[:cutoff], Y[:cutoff]\n",
    "X_gl_test, Y_test = X_gl[cutoff:], Y[cutoff:]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_iter = DataIteratorEmb(X_gl_train, Y_train)\n",
    "    cur_epoch = train_iter.epoch\n",
    "    cur_loss = 0.0\n",
    "    while train_iter.epoch < num_epochs:\n",
    "        batch_x_gl, batch_y = train_iter.next_batch(batch_size)\n",
    "        feed = {x_gl:batch_x_gl, y:batch_y}\n",
    "        [_, batch_loss] = sess.run([train_op, loss],\n",
    "                                   feed_dict=feed)\n",
    "        cur_loss += batch_loss\n",
    "        if cur_epoch<train_iter.epoch:\n",
    "            print 'Epoch %d loss = %.2f' % (cur_epoch, cur_loss)\n",
    "            cur_epoch = train_iter.epoch\n",
    "            cur_loss = 0\n",
    "    print\n",
    "    \n",
    "    # Eval on train\n",
    "    num_correct = sess.run(correct, feed_dict={x_gl:X_gl_train, y:Y_train})\n",
    "    print \"Accuracy on train: \" + repr(num_correct/float(len(X_so_train)))\n",
    "    \n",
    "    # Eval on test\n",
    "    num_correct = sess.run(correct, feed_dict={x_gl:X_gl_test, y:Y_test})\n",
    "    print \"Accuracy on train: \" + repr(num_correct/float(len(X_so_test)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding + WK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 98.34\n",
      "Epoch 1 loss = 90.94\n",
      "Epoch 2 loss = 87.12\n",
      "Epoch 3 loss = 84.43\n",
      "Epoch 4 loss = 81.90\n",
      "Epoch 5 loss = 80.41\n",
      "Epoch 6 loss = 79.10\n",
      "Epoch 7 loss = 77.50\n",
      "Epoch 8 loss = 76.24\n",
      "Epoch 9 loss = 75.31\n",
      "Epoch 10 loss = 74.39\n",
      "Epoch 11 loss = 73.49\n",
      "Epoch 12 loss = 72.91\n",
      "Epoch 13 loss = 72.25\n",
      "Epoch 14 loss = 71.55\n",
      "Epoch 15 loss = 70.95\n",
      "Epoch 16 loss = 70.31\n",
      "Epoch 17 loss = 70.17\n",
      "Epoch 18 loss = 69.39\n",
      "Epoch 19 loss = 69.05\n",
      "\n",
      "Accuracy on train: 0.76261343012704175\n",
      "Accuracy on train: 0.76872964169381108\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "num_epochs = 20\n",
    "batch_size = 20\n",
    "\n",
    "emb_size = 10\n",
    "hidden_size = 10\n",
    "glove_size = 900\n",
    "num_classes = 2\n",
    "\n",
    "init_lr = 1e-4\n",
    "keep = 1.0\n",
    "\n",
    "def get_feat_size(scheme):\n",
    "    return len(featbindiff_indexer) if scheme=='bin' else len(feat3lvdiff_indexer)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "x_so = tf.placeholder(tf.int32, [None, 6])\n",
    "E_so = tf.get_variable('E-so', [get_feat_size(scheme), emb_size],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "e_so = tf.nn.embedding_lookup(E_so, x_so) # (batch_size, 6, 10)\n",
    "\n",
    "x_v = tf.placeholder(tf.int32, [None, 1])\n",
    "E_v = tf.get_variable('E-v', [len(v_indexer), emb_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "e_v = tf.nn.embedding_lookup(E_v, x_v) # (batch_size, 1, 10)\n",
    "\n",
    "e_svo = tf.contrib.layers.flatten(tf.concat((e_so, e_v), axis=1)) \n",
    "    # concat: (batch_size, 7, 10)\n",
    "    # flatten: 7 10D embeddings gets concatenated [10,10,10,...] (does not touch None)\n",
    "\n",
    "W1 = tf.get_variable('W1', [emb_size*7, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable('b1', [hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "z1 = tf.nn.relu(tf.add(tf.matmul(e_svo, W1), b1))\n",
    "\n",
    "x_gl = tf.placeholder(tf.float32, [None, glove_size])\n",
    "\n",
    "W2 = tf.get_variable('W2', [glove_size, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable('b2', [hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "z2 = tf.nn.relu(tf.add(tf.matmul(x_gl, W2), b2))\n",
    "\n",
    "z = tf.nn.dropout(tf.concat((z1, z2), axis=1), keep_prob)\n",
    "\n",
    "W = tf.get_variable('W', [hidden_size*2, num_classes],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b', [num_classes],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "probs = tf.nn.softmax(tf.add(tf.matmul(z, W), b))\n",
    "preds = tf.cast(tf.argmax(probs, 1), dtype=tf.int32)\n",
    "\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "y_onehot = tf.one_hot(y, 2)\n",
    "\n",
    "loss = tf.reduce_mean(tf.negative(tf.log(tf.reduce_sum(tf.multiply(probs, y_onehot), 1))))\n",
    "\n",
    "# eq = tf.equal(preds, y)\n",
    "correct = tf.reduce_sum(tf.cast(tf.equal(preds, y), dtype=tf.int32))\n",
    "\n",
    "decay_steps = 100\n",
    "learning_rate_decay_factor = 0.95\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "initial_learning_rate = init_lr\n",
    "lr = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                global_step,\n",
    "                                decay_steps,\n",
    "                                learning_rate_decay_factor,\n",
    "                                staircase=True)\n",
    "# optimizer setup\n",
    "opt = tf.train.AdamOptimizer(lr)\n",
    "grads = opt.compute_gradients(loss)\n",
    "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "with tf.control_dependencies([apply_gradient_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "# run\n",
    "init = tf.global_variables_initializer()\n",
    "cutoff = int(len(svo2plau)*train_ratio)\n",
    "init_indices = np.arange(len(svo2plau))\n",
    "random.shuffle(init_indices)\n",
    "X_so = X_so[init_indices]\n",
    "X_v = X_v[init_indices]\n",
    "X_gl = X_gl[init_indices]\n",
    "Y = Y[init_indices]\n",
    "X_so_train, X_v_train, X_gl_train, Y_train = X_so[:cutoff], X_v[:cutoff], X_gl[:cutoff], Y[:cutoff]\n",
    "X_so_test, X_v_test, X_gl_test, Y_test = X_so[cutoff:], X_v[cutoff:], X_gl[cutoff:], Y[cutoff:]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_iter = DataIteratorEmbWK(X_so_train, X_v_train, X_gl_train, Y_train)\n",
    "    cur_epoch = train_iter.epoch\n",
    "    cur_loss = 0.0\n",
    "    while train_iter.epoch < num_epochs:\n",
    "        batch_x_so, batch_x_v, batch_x_gl, batch_y = train_iter.next_batch(batch_size)\n",
    "        feed = {x_so:batch_x_so, x_v:batch_x_v, x_gl:batch_x_gl, \n",
    "                y:batch_y, keep_prob:keep}\n",
    "        [_, batch_loss] = sess.run([train_op, loss],\n",
    "                                   feed_dict=feed)\n",
    "        cur_loss += batch_loss\n",
    "        if cur_epoch<train_iter.epoch:\n",
    "            print 'Epoch %d loss = %.2f' % (cur_epoch, cur_loss)\n",
    "            cur_epoch = train_iter.epoch\n",
    "            cur_loss = 0\n",
    "    print\n",
    "    \n",
    "    # Eval on train\n",
    "    num_correct = sess.run(correct, feed_dict={x_so:X_so_train,\n",
    "                                               x_v:X_v_train,\n",
    "                                               x_gl:X_gl_train,\n",
    "                                               y:Y_train,\n",
    "                                               keep_prob:1.0})\n",
    "    print \"Accuracy on train: \" + repr(num_correct/float(len(X_so_train)))\n",
    "    \n",
    "    # Eval on test\n",
    "    num_correct = sess.run(correct, feed_dict={x_so:X_so_test,\n",
    "                                               x_v:X_v_test,\n",
    "                                               x_gl:X_gl_test,\n",
    "                                               y:Y_test,\n",
    "                                               keep_prob:1.0})\n",
    "    print \"Accuracy on train: \" + repr(num_correct/float(len(X_so_test)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "# TF interactive testing\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# # x = tf.placeholder(tf.int32, [None, 6])\n",
    "# # emb_mat = tf.get_variable('emb', [20, 10])\n",
    "# # emb = tf.nn.embedding_lookup(emb_mat, x) # (batch_size, n_col, emb_size)\n",
    "\n",
    "# x1 = tf.placeholder(tf.int32, [3,2])\n",
    "# x1_float = tf.placeholder(tf.float32, [3,2])\n",
    "# x2 = tf.placeholder(tf.int32, [1,2])\n",
    "# x = tf.concat((x1,x2), axis=0)\n",
    "# x_reshape = tf.reshape(x, [-1])\n",
    "\n",
    "# sm = tf.nn.softmax(x1_float)\n",
    "\n",
    "# a1 = np.array([[1,1],\n",
    "#               [2,2],\n",
    "#               [3,3]])\n",
    "# a1_float = np.array([[1,1],\n",
    "#                       [2,2],\n",
    "#                       [3,3]],dtype=np.float32)\n",
    "# a2 = np.array([[4,4]])\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# # d = sess.run(x, feed_dict={x1:a1,x2:a2})\n",
    "# # e = sess.run(x_reshape, feed_dict={x1:a1,x2:a2})\n",
    "# print sess.run(sm, feed_dict={x1_float:a1_float})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
